default_model:
  url:  # Will be read from OPENAI_BASE_URL environment variable
  api_key:  # Will be read from OPENAI_API_KEY environment variable
  model_name: qwen-plus
  temperature: 0.7
  max_tokens: 2048
  stream: false
  trainable: false
  act_by_user: false
  language: en
  # Default agent_config for all roles
  agent_config:
    type: ThinkingReActAgent
    kwargs:
      memory:
        type: SlidingWindowMemory
        kwargs: {}

# Default game configuration
game:
  name: diplomacy
  power_names:
    - AUSTRIA
    - ENGLAND
    - FRANCE
    - GERMANY
    - ITALY
    - RUSSIA
    - TURKEY
  map_name: standard
  max_phases: 12
  negotiation_rounds: 3
  seed: 42
  language: zh  # zh: Chinese, en: English
  log_dir: logs

# Power-specific configuration (empty by default, can be overridden in child configs)
# 
# Configuration Priority:
#   1. Power-specific settings in 'roles' section (if specified)
#   2. 'default' in 'roles' section (if specified)
#   3. Default settings in 'default_model' section
#
# Agent Configuration (agent_config):
#   Use agent_config to specify agent type and all components (memory, formatter, etc.)
#   See games/games/avalon/configs/default_config.yaml for detailed agent_config documentation.
#
#   Examples:
#     # Example 1: Power with custom agent
#     ENGLAND:
#       url: http://localhost:8000/v1  # VLLM server URL
#       model_name: local_model
#       agent_config:
#         type: YourCustomAgent
#         kwargs:
#           sys_prompt: ""
#           memory:
#             type: SlidingWindowMemory
#             kwargs: {}
#
#     # Example 2: Power with default agent_config (inherits from default_model)
#     FRANCE:
#       model_name: qwen-max
#       # agent_config will use default_model.agent_config if not specified

roles:
  # Example 1: Power with local VLLM server
  # ENGLAND:
  #   url: http://localhost:8000/v1  # VLLM server URL (start with: python games/evaluation/start_vllm.py --model-path /path/to/model)
  #   model_name: local_model  # Model name served by VLLM (must match --model-name in start_vllm.py)
  #   # temperature: 0.7  # Optional, uses default_model configuration
  #   # max_tokens: 2048  # Optional
  #   agent_config:
  #     type: YourCustomAgent
  #     kwargs:
  #       sys_prompt: ""
  
  # Example 2: Power with custom model (uses default agent_config)
  # FRANCE:
  #   model_name: qwen-max
  
  default:
    model_name: qwen-plus